{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2335, 34)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_24</th>\n",
       "      <th>sensor_25</th>\n",
       "      <th>sensor_26</th>\n",
       "      <th>sensor_27</th>\n",
       "      <th>sensor_28</th>\n",
       "      <th>sensor_29</th>\n",
       "      <th>sensor_30</th>\n",
       "      <th>sensor_31</th>\n",
       "      <th>sensor_32</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-6.149463</td>\n",
       "      <td>-0.929714</td>\n",
       "      <td>9.058368</td>\n",
       "      <td>-7.017854</td>\n",
       "      <td>-2.958471</td>\n",
       "      <td>0.179233</td>\n",
       "      <td>-0.956591</td>\n",
       "      <td>-0.972401</td>\n",
       "      <td>5.956213</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.026436</td>\n",
       "      <td>-6.006282</td>\n",
       "      <td>-6.005836</td>\n",
       "      <td>7.043084</td>\n",
       "      <td>21.884650</td>\n",
       "      <td>-3.064152</td>\n",
       "      <td>-5.247552</td>\n",
       "      <td>-6.026107</td>\n",
       "      <td>-11.990822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-2.238836</td>\n",
       "      <td>-1.003511</td>\n",
       "      <td>5.098079</td>\n",
       "      <td>-10.880357</td>\n",
       "      <td>-0.804562</td>\n",
       "      <td>-2.992123</td>\n",
       "      <td>26.972724</td>\n",
       "      <td>-8.900861</td>\n",
       "      <td>-5.968298</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.996714</td>\n",
       "      <td>-7.933806</td>\n",
       "      <td>-3.136773</td>\n",
       "      <td>8.774211</td>\n",
       "      <td>10.944759</td>\n",
       "      <td>9.858186</td>\n",
       "      <td>-0.969241</td>\n",
       "      <td>-3.935553</td>\n",
       "      <td>-15.892421</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>19.087934</td>\n",
       "      <td>-2.092514</td>\n",
       "      <td>0.946750</td>\n",
       "      <td>-21.831788</td>\n",
       "      <td>9.119235</td>\n",
       "      <td>17.853587</td>\n",
       "      <td>-21.069954</td>\n",
       "      <td>-15.933212</td>\n",
       "      <td>-9.016039</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.889685</td>\n",
       "      <td>54.052330</td>\n",
       "      <td>-6.109238</td>\n",
       "      <td>12.154595</td>\n",
       "      <td>6.095989</td>\n",
       "      <td>-40.195088</td>\n",
       "      <td>-3.958124</td>\n",
       "      <td>-8.079537</td>\n",
       "      <td>-5.160090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-2.211629</td>\n",
       "      <td>-1.930904</td>\n",
       "      <td>21.888406</td>\n",
       "      <td>-3.067560</td>\n",
       "      <td>-0.240634</td>\n",
       "      <td>2.985056</td>\n",
       "      <td>-29.073369</td>\n",
       "      <td>0.200774</td>\n",
       "      <td>-1.043742</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.126170</td>\n",
       "      <td>-1.035526</td>\n",
       "      <td>2.178769</td>\n",
       "      <td>10.032723</td>\n",
       "      <td>-1.010897</td>\n",
       "      <td>-3.912848</td>\n",
       "      <td>-2.980338</td>\n",
       "      <td>-12.983597</td>\n",
       "      <td>-3.001077</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3.953852</td>\n",
       "      <td>2.964892</td>\n",
       "      <td>-36.044802</td>\n",
       "      <td>0.899838</td>\n",
       "      <td>26.930210</td>\n",
       "      <td>11.004409</td>\n",
       "      <td>-21.962423</td>\n",
       "      <td>-11.950189</td>\n",
       "      <td>-20.933785</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.051761</td>\n",
       "      <td>10.917567</td>\n",
       "      <td>1.905335</td>\n",
       "      <td>-13.004707</td>\n",
       "      <td>17.169552</td>\n",
       "      <td>2.105194</td>\n",
       "      <td>3.967986</td>\n",
       "      <td>11.861657</td>\n",
       "      <td>-27.088846</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   sensor_1  sensor_2   sensor_3   sensor_4   sensor_5   sensor_6  \\\n",
       "0   1  -6.149463 -0.929714   9.058368  -7.017854  -2.958471   0.179233   \n",
       "1   2  -2.238836 -1.003511   5.098079 -10.880357  -0.804562  -2.992123   \n",
       "2   3  19.087934 -2.092514   0.946750 -21.831788   9.119235  17.853587   \n",
       "3   4  -2.211629 -1.930904  21.888406  -3.067560  -0.240634   2.985056   \n",
       "4   5   3.953852  2.964892 -36.044802   0.899838  26.930210  11.004409   \n",
       "\n",
       "    sensor_7   sensor_8   sensor_9  ...  sensor_24  sensor_25  sensor_26  \\\n",
       "0  -0.956591  -0.972401   5.956213  ...  -7.026436  -6.006282  -6.005836   \n",
       "1  26.972724  -8.900861  -5.968298  ...  -1.996714  -7.933806  -3.136773   \n",
       "2 -21.069954 -15.933212  -9.016039  ...  -6.889685  54.052330  -6.109238   \n",
       "3 -29.073369   0.200774  -1.043742  ...  -2.126170  -1.035526   2.178769   \n",
       "4 -21.962423 -11.950189 -20.933785  ...  -2.051761  10.917567   1.905335   \n",
       "\n",
       "   sensor_27  sensor_28  sensor_29  sensor_30  sensor_31  sensor_32  target  \n",
       "0   7.043084  21.884650  -3.064152  -5.247552  -6.026107 -11.990822       1  \n",
       "1   8.774211  10.944759   9.858186  -0.969241  -3.935553 -15.892421       1  \n",
       "2  12.154595   6.095989 -40.195088  -3.958124  -8.079537  -5.160090       0  \n",
       "3  10.032723  -1.010897  -3.912848  -2.980338 -12.983597  -3.001077       1  \n",
       "4 -13.004707  17.169552   2.105194   3.967986  11.861657 -27.088846       2  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "tf.random.set_seed(42) #재현을 위한 텐서플로우 seed 설정\n",
    "\n",
    "data_train = pd.read_csv('./train.csv')\n",
    "data_test = pd.read_csv('./test.csv')\n",
    "print(data_train.shape)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 보게되면 train data의 수가 2335로 매우 적으며, test data의 수가 train data의 약 5배 정도로 많은 것을 볼 수 있습니다.\n",
    "\n",
    "따라서 저는 처음에 모델 튜닝보다는, train data의 수를 늘리는 방법에 대해 고민하는것이 훨씬 이득이라고 생각해서 train data를 증강해보기 위해서 다양하게 접근하였으나, 모두 실패하였습니다. (여기서 너무 많은 시간을 소모했습니다... sensor가 어떤 센서인지 도통 알수가 없네요)\n",
    "\n",
    "현재 사용하고 있는 증강법은 데이터의 노이즈를 추가하는 방법 중 swap noise 하나밖에 없습니다. swap noise에 대해서는 아래 블로그에 잘 나와있습니다.\n",
    "\n",
    "https://velog.io/@vvakki_/Tabular-Data%EC%A0%95%ED%98%95-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%90%EC%84%9C%EC%9D%98-Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [\"data_train\", \"data test\"]\n",
    "y = [data_train.shape[0], data_test.shape[0]]\n",
    "ax = sns.barplot(x=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "relu 함수를 activation에서 사용하고 있기 때문에 음수 값을 모두 양수로 만들어 dying relu를 만들지 않도록 했습니다.\n",
    "\n",
    "또한 기존에 하던 것처럼 0~1값으로 Normalization을 진행해 입력의 범위를 맞춰주었으며, 값의 첨도가 높아 log변환을 시도해보았지만 성능이 더욱 좋지 않게 나와 그냥 나눠주었습니다.\n",
    "\n",
    "https://yamalab.tistory.com/48 \n",
    "\n",
    "위 블로그에서 발췌한 글입니다. \n",
    "\n",
    "input값에 음수가 포함이 된다면 기울기가 0이 되버리므로, 미분을 하면 backpropagation 과정 중간에 꺼져버리는 상황이 발생한다. Chain rule로 미분을 하기 때문에 음수가 한번 나오면 뒤에서도 다 꺼진다. 따라서 input 데이터에서 음수값이 포함되지 않도록 0~1사이의 값으로 정규화 시키는 과정을 거치는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = data_train.drop(['target', 'id'], axis = 1)\n",
    "train_X = (train_X+130)/260 \n",
    "train_X = np.array(train_X)\n",
    "train_X = np.array(train_X).reshape(-1, 8, 4, 1)\n",
    "\n",
    "train_Y = data_train['target']\n",
    "train_Y = np.array(train_Y)\n",
    "\n",
    "X_test = data_test.drop('id', axis = 1)\n",
    "X_test = (X_test+130)/260\n",
    "X_test = np.array(X_test).reshape(-1, 8, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min : 0.008470580769230797 \n",
      "max : 0.9890809815384616\n"
     ]
    }
   ],
   "source": [
    "print(\"min :\", train_X.min(), \"\\nmax :\", train_X.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2335, 8, 4, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2335,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 생성하기\n",
    "\n",
    "\n",
    "기세현님이 공유해주신 방식처럼 CNN으로 예측하는 방식이 가장 성능이 좋게 나와 저도 CNN을 기반으로 접근해보았습니다.\n",
    "\n",
    "https://dacon.io/codeshare/4653\n",
    "\n",
    "모델은 이전에 제가 사물 이미지 분류 경진대회에서 만들었던 ResNet 구조에, bottleneck 구조를 살려서 구현한 모델을 사용했습니다.\n",
    "\n",
    "https://dacon.io/codeshare/4618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, filters, kernel_size):\n",
    "    X_shortcut = X\n",
    "    \n",
    "    X = tf.keras.layers.Conv2D(filters, (1,1), padding='SAME')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.Conv2D(filters, kernel_size, padding='SAME')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.Conv2D(filters*4, (1,1), padding='SAME')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    \n",
    "    # Add\n",
    "    X = tf.keras.layers.Add()([X, X_shortcut])\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, filters, kernel_size):\n",
    "    X_shortcut = X\n",
    "    \n",
    "    X = tf.keras.layers.Conv2D(filters, (1,1), padding='SAME')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.Conv2D(filters, kernel_size, padding='SAME')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    X = tf.keras.layers.Conv2D(filters*4, (1,1), padding='SAME')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "\n",
    "    X_shortcut = tf.keras.layers.Conv2D(filters*4, (1,1), padding='SAME')(X_shortcut) #use 1x1 conv to make shape same\n",
    "    X_shortcut = tf.keras.layers.BatchNormalization()(X_shortcut)\n",
    "    \n",
    "    # Add\n",
    "    X = tf.keras.layers.Add()([X, X_shortcut])\n",
    "    X = tf.keras.layers.Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구글에 cnn rectangular images라고 검색하니 나온 이 article에서 음성 신호에서 감정을 인식하기 위해, 음성 신호를 image로 변환한 직사각형 input을 사용하고 있습니다. 여기서 구현한 모델을 보면 직사각형 input에는 직사각형 필터를 사용하고 있기 때문에 저도 **(3,2) (2,1) 처럼 직사각형 필터를 사용했습니다.** (성능이 향상되었습니다.)\n",
    "\n",
    "https://www.researchgate.net/publication/320737885_Deep_features-based_speech_emotion_recognition_for_smart_affective_services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://www.researchgate.net/profile/Jamil-Ahmad-13/publication/320737885/figure/fig2/AS:941749673414722@1601542027738/Proposed-CNN-architecture-with-rectangular-kernels.gif>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomModel(input_shape = (8, 4, 1), classes = 4):\n",
    "    X_input = tf.keras.layers.Input(input_shape)\n",
    "    X = X_input\n",
    "    \n",
    "    X = convolutional_block(X, 128, (3,2)) #(3,3) 보다는 직사각형 이미지이기때문에 (3,2)처럼 직사각형 필터를 사용\n",
    "    X = identity_block(X, 128, (3,2))\n",
    "    X = identity_block(X, 128, (3,2))\n",
    "    \n",
    "    X = tf.keras.layers.AveragePooling2D(2,2)(X) #Max보다는 Average pool이 성능이 잘나옴\n",
    "\n",
    "    X = convolutional_block(X, 256, (2,1)) #(2,1) 직사각형 필터를 사용\n",
    "    X = identity_block(X, 256, (2,1))\n",
    "    X = identity_block(X, 256, (2,1))\n",
    "    \n",
    "    X = tf.keras.layers.GlobalAveragePooling2D()(X) #Flatten 대신 사용\n",
    "    \n",
    "    X = tf.keras.layers.Dense(128, activation = \"relu\")(X)\n",
    "    \n",
    "    X = tf.keras.layers.Dropout(0.5)(X)\n",
    "    \n",
    "    X = tf.keras.layers.Dense(classes, activation = \"softmax\")(X)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = X_input, outputs = X, name = \"CustomModel\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습하기\n",
    "\n",
    "우선 위에서 설명했던것처럼 train set이 너무 적고, test 셋이 train set에 비해 너무 많기 때문에 k-fold를 이용한 예측값을 사용하여 앙상블하게되면, 성능이 높게 올라가는 것을 알 수 있습니다.\n",
    "\n",
    "따라서 train셋을 조금이라도 늘릴 방법에 대해서 중점적으로 접근해보았고, 많은 실패를 겪고 결국 swap noise 하나만 사용하게 되었습니다.\n",
    "\n",
    "또한 Fold 횟수를 15번으로 설정한 것이 가장 점수가 높게 나와, 15번의 fold를 진행하도록 했습니다. (train data set이 너무 적어서 그런것 같습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LastDefiance\\anaconda3\\envs\\ssu\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1 / val_accuracy : 0.8910 / val_loss : 0.6203\n",
      "fold 2 / val_accuracy : 0.8333 / val_loss : 0.4866\n",
      "fold 3 / val_accuracy : 0.8782 / val_loss : 0.7207\n",
      "fold 4 / val_accuracy : 0.8974 / val_loss : 0.6771\n",
      "fold 5 / val_accuracy : 0.9038 / val_loss : 0.4689\n",
      "fold 6 / val_accuracy : 0.8590 / val_loss : 0.5532\n",
      "fold 7 / val_accuracy : 0.8654 / val_loss : 0.5898\n",
      "fold 8 / val_accuracy : 0.8974 / val_loss : 0.8402\n",
      "fold 9 / val_accuracy : 0.8782 / val_loss : 0.6116\n",
      "fold 10 / val_accuracy : 0.8782 / val_loss : 0.4823\n",
      "fold 11 / val_accuracy : 0.8581 / val_loss : 0.9745\n",
      "fold 12 / val_accuracy : 0.8258 / val_loss : 0.8205\n",
      "fold 13 / val_accuracy : 0.8968 / val_loss : 0.7462\n",
      "fold 14 / val_accuracy : 0.8968 / val_loss : 0.8990\n",
      "fold 15 / val_accuracy : 0.8903 / val_loss : 0.7271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 15, random_state = 42, shuffle = True) #총 15번의 fold 진행\n",
    "n = 0 #x번째 fold인지 기록\n",
    "\n",
    "cnn_pred = [] #모델의 예측값 모두 저장\n",
    "\n",
    "for train_index, valid_index in skf.split(train_X, train_Y):\n",
    "    n += 1\n",
    "    X_train, X_valid = train_X[train_index], train_X[valid_index]\n",
    "    y_train, y_valid = train_Y[train_index], train_Y[valid_index]\n",
    "\n",
    "    ### Swap Noise ###\n",
    "    X_train_mix = np.array(X_train)\n",
    "    for x in range(X_train_mix.shape[0]):\n",
    "        for i in range(5):\n",
    "            y = np.random.randint(0, 8)\n",
    "            z = np.random.randint(0, 4)\n",
    "            \n",
    "            while True:\n",
    "                c = np.random.randint(0, X_train_mix.shape[0]-1)\n",
    "                if ((x != c)and(y_train[x] == y_train[c])):\n",
    "                    break\n",
    "                    \n",
    "            X_train_mix[x][y][z] = X_train[c][y][z]\n",
    "\n",
    "    X_train = np.append(X_train, X_train_mix, axis = 0)\n",
    "    y_train = np.append(y_train, y_train, axis = 0)\n",
    "    \n",
    "    ### Mix Data Again ####\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "    \n",
    "    y_train = tf.one_hot(y_train, 4)\n",
    "    y_train = tf.reshape(y_train, [-1,4])\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    y_valid = tf.one_hot(y_valid, 4)\n",
    "    y_valid = tf.reshape(y_valid, [-1,4])\n",
    "    y_valid = np.array(y_valid)\n",
    "    \n",
    "    ### Create Model ###\n",
    "    model = CustomModel()\n",
    "    \n",
    "    ### Compile Model ###\n",
    "    model.compile(optimizer='adam', # 무난한 adam 사용\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    ### Create callbacks ###\n",
    "    filename = 'CNN-checkpoint.h5'\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filename,             # file명을 지정합니다\n",
    "                                                    monitor='val_accuracy',   # val_accuracy 값이 개선되었을때 호출됩니다\n",
    "                                                    verbose=0,            # 로그를 출력합니다 0일경우 출력 X\n",
    "                                                    save_best_only=True,  # 가장 best 값만 저장합니다\n",
    "                                                    mode='auto'           # auto는 알아서 best를 찾습니다. min/max (loss->min, accuracy->max)\n",
    "                                                   )\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',  # 모니터 기준 설정 (val loss) \n",
    "                                  patience=12,         # 12 Epoch동안 개선되지 않는다면 종료\n",
    "                                 )\n",
    "    reduceLR = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_accuracy', # val_accuracy 값이 개선되었을때 호출됩니다\n",
    "        factor=0.5, # learning rate이 0.5배 줄어듬\n",
    "        patience=6, # 6 Epoch동안 개선되지 않는다면 호출\n",
    "    )\n",
    "    \n",
    "    ### fit model ###\n",
    "    data = model.fit(X_train, \n",
    "                     y_train, \n",
    "                     validation_data=(X_valid, y_valid), \n",
    "                     epochs=60, \n",
    "                     batch_size=32, # batch size가 32일때 가장 좋은 성능을 보임\n",
    "                     callbacks=[reduceLR, earlystopping, checkpoint],\n",
    "                     verbose=0 # 로그 출력을 없앰, 어짜피 아래 print에서 한번에 best_accuracy만 출력할것이기 때문이다.\n",
    "                    )\n",
    "    \n",
    "    idx = data.history['val_accuracy'].index(max(data.history['val_accuracy']))\n",
    "    \n",
    "    print(\"fold %d / val_accuracy : %0.4f / val_loss : %0.4f\" %(n,\n",
    "                                                                data.history['val_accuracy'][idx], \n",
    "                                                                data.history['val_loss'][idx]))\n",
    "    \n",
    "    ### predict model ###\n",
    "    model = tf.keras.models.load_model('./CNN-checkpoint.h5') # best accuracy를 기록한 모델을 불러옴\n",
    "    pred_proba = model.predict(X_test) # 테스트 셋에 대한 예측 수행\n",
    "    cnn_pred.append(pred_proba) # 예측값을 cnn_pred 리스트에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제출하기\n",
    "\n",
    "이전에 계산했던 model.predict 값이 저장된 cnn_pred 리스트에 있는 값을 모두 더한 후, argmax를 이용하여 예측값을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.9998200e-01 5.9752169e-06 8.7663730e-06 3.2298253e-06]\n",
      "[0.97598696 0.00577842 0.01719919 0.00103535]\n",
      "[0.9768521  0.00525254 0.01474087 0.00315449]\n",
      "[1.0000000e+00 1.9110527e-10 1.9541596e-11 1.0021416e-09]\n",
      "[9.9918634e-01 8.4004132e-06 7.8036642e-04 2.4830451e-05]\n",
      "[9.99999285e-01 1.43209915e-08 5.48386652e-07 1.61913192e-07]\n",
      "[9.9996364e-01 5.4064834e-08 3.4444300e-05 1.8961626e-06]\n",
      "[9.9999964e-01 2.1311675e-07 7.5045534e-08 2.2922140e-09]\n",
      "[9.9991524e-01 6.3283274e-05 2.1003960e-05 4.4668801e-07]\n",
      "[9.8618466e-01 3.4116080e-04 1.1958394e-02 1.5157476e-03]\n",
      "[9.9999690e-01 5.5355116e-08 2.6908785e-06 4.6398733e-07]\n",
      "[9.9849272e-01 1.3228181e-04 4.9938250e-04 8.7562570e-04]\n",
      "[9.9998605e-01 6.1045571e-06 7.6500637e-06 2.0100140e-07]\n",
      "[1.0000000e+00 4.6204334e-12 2.1243608e-12 6.1020972e-10]\n",
      "[9.9999750e-01 7.5454857e-08 2.0532323e-07 2.1380079e-06]\n"
     ]
    }
   ],
   "source": [
    "pred_proba = cnn_pred[0]\n",
    "pred_proba = np.array(pred_proba)\n",
    "print(pred_proba[0])\n",
    "\n",
    "for x in range(1, 15):\n",
    "    pred_proba += cnn_pred[x]\n",
    "    print(cnn_pred[x][0])\n",
    "\n",
    "pred_class = []\n",
    "\n",
    "for i in pred_proba:\n",
    "    pred = np.argmax(i)\n",
    "    pred_class.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "sample_submission.target = pred_class\n",
    "sample_submission.to_csv(\"submit_25.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   1       0\n",
       "1   2       0\n",
       "2   3       1\n",
       "3   4       3\n",
       "4   5       2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ssu]",
   "language": "python",
   "name": "conda-env-ssu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
